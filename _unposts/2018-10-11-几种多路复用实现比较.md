---
layout:     post
title:      几种多路复用实现比较
date:       2018-10-11
author:     loloseed
header-img: img/post-bg-cook.jpg
catalog: 	 true
tags:
    - 网络
---
当有大量并发请求时，“同时”处理大量并发请求。多路复用技术的出现解决了这个问题。
有4种方式来解决应用程序阻塞等待某一个fd的IO事件，使得服务端能同时响应更多的并发请求：
> 1.把fd设置成noblocking模式，应用程序轮逻该fd IO是否ready。
> 2.当被关注的fd ready的时候，发送信号给应用程序。
> 3.操作系统提供方法，返回ready for IO的fd。
> 4.应用程序对关注的fd的IO事件向操作系统注册回调函数。

第1种方式，这种忙轮逻导致100%CPU利用率，这种方式不实际。
第2种方式，当捕获SIGIO信号时，应用程序是不知道哪个fd是ready的。
第3种方式，目前select/poll提供的方式。应用程序把关心的fd传入后被挂起，等有ready的fd时被唤起。
第4种方式，目前有freeBSD的kqueue，windows的IOCP，linux下面有epoll。
# select
select有2个被受争议的问题。
## 开销大
当进程调用完select后，内核会遍历所有传入的fds，检查fd的状态，注册回调函数。当任意一个fd发生IO事件，内核再次遍历所有的fds，注销回调函数。进入和离开select时遍历所有的fd指针这个操作开销很大，会导致处理器缓存被覆盖，频繁访问内存。
## 惊群效应
当同一fd被多个进程/线程共享时。比如共享一个listen socket，
# epoll
1994年IOCP，2000年kqueue，epoll是2002年出来的。仅管epoll最晚开发出来，但是在这几个里面是最差的。
关于epoll一直有2个争议。
- 多线程扩展问题。最早的时候，epoll和select一样，有惊群效应的问题。当多个线程共享一个epoll fd时，不能把该fd上的发生的事件单独发送给一个线程。这个问题现在可以通过fcntl设置EPOLLONESHOT和EPOLLEXCLUSIVE标识解决。
- 注册的事件没有关联到用户态数据结构 **文件描述符**fd上，而是关联到fd指向的一个内核数据结构 **文件描述**上。

这2个争议是可以通过技术手段解决。所以：
- 不要用epoll解决多线程之间的负载均衡问题。
- 多线程之间不要共享epoll fd。
- 不要共享注册到epoll fd的fd（比如accept的fd）。
- 避免fork，如果必须要fork，在fork之前关闭所有注册到epoll fd的fd。
- 在dup/close epoll fd之前，注销掉这个epoll fd上注册的所有fd。
## 负载均衡
有2个需要负载均衡的地方。
- 对唯一的tcp listen套接字，accept()的时候扩展。
- 大量并发请求，read()的时候需要扩展。
### 扩展accept()
当有大量并发的连接请求时，希望是把accept连接的工作均匀地分布到多个核上并行执行。比如真实场景里每秒40k的连接请求，靠单一线程accept是处理不了了。
为了使多线程accept连接，简单的方法是，1）多个线程共享同一个epoll fd；2）线程有自己独立的epoll fd，共享listen socket。这2种方式都有问题。唤起线程accept()连接有2种模式，level-triggerd和edge-triggered。
#### level-triggered - 水平触发，引发不必要的线程唤起
单纯level-triggered模式，epoll fd如果不设置特殊标识，存在惊群效应。每次有新的连接都会导致所有线程被唤起。举个例子：
> 1. **kernel**：收到一个连接请求。
> 2. **kernel**：唤起等待IO事件的线程A和B。
> 3. **thread A**: accept()成功。
> 4. **thread B**: accept()失败，返回EAGAIN。
这里，唤起B线程是不必要的，造成资源浪费，这种方式扩展性很差。
#### edge-triggered - 边缘触发，引发不必要的线程唤起、饥饿
既然level-triggered会唤起所有的线程，导致CPU资源浪费，CPU缓存热点数据换出。那我们试试edge-triggered模式，是不是解决问题了呢？答案是并没有，edge-triggered仍然会引起不必要的线程唤起。看看这种造成不必要唤起的情况：
> 1. **kernel**: 收到连接请求。有线程A和B等待IO事件，假设线程A被唤起（edge-triggered模式只会唤起一起）。
> 2. **thread A**: epoll_wait()返回。
> 3. **thread A**: accept()成功。
> 4. **kernel**: accept队列变空，socket由readble可读状态变为non-readable不可讯状态。
> 5. **kernel**: 收到新的连接请求。
> 6. **kernel**: 只有线程B在等待该socket事件，因此唤起线程B。
> 7. **thread A**: edge-triggered模式多个连接请求只通知一次，为了不漏掉请求，会继续调用accept()成功。
> 8. **thread B**: accept()返回EAGAIN，被唤起但是没有读取到数据。
> 9. **thread A**: 继续调用accept()返回EAGAIN。

唤起线程B是浪费的。下面再举个极端的例子，edge-triggered会造成饥饿：
> 1. **kernel**: 收到2个连接请求。线程A和B同时等待，因为是edge-triggered只唤起一次，假设线程A被唤起。
> 2. **thread A**: epoll_wait()结束，accept()成功。
> 3. **kernel**: 收到第3次连接请求，因有未处理完的连接请求，socket保持readable状态不变，不会触发事件唤起线程。
> 4. **thread A**: 继续accept()成功。
> 5. **kernel**: 继续收到连接请求。
> 6. **thread A**: 继续accept()成功。

这个场景下，socket只有一次从non-readable状态变为readable状态，因为是edge-triggered模式，内核只唤起一个epoll_wait，所有的连接请求都被发送到thread A处理，负载就不均衡了。

#### 解决方案
accept()的扩展性能不能解决呢？4.5以后的内核版本提供了3种办法。
1. level-triggered事件+EPOLLEXCLUSIVE标识。各线程创建自己的epoll fd，监听同一个listen fd上的事件。当发生唤醒事件，内核会遍历绑定在 listen fd的epoll fd，事件加入到epoll fd的事件队列，如果拥有epoll fd的线程空闲等待则结束；否则继续把事件加入到epoll fd的队列。这样可以减少被换起的线程数，一个到多个线程被唤起。
2. edge-triggered事件+EPOLLONESHOT标识，但是需要在事件发出后多调用一次系统调用epoll_ctl()。这样可以利用多核把accept()操作交给多个线程处理，并且不会引发惊群效应。但是需要前一个线程accept()完后，调用epoll_ctl设置EPOLLONESHOT，才会触发下个事件，所以同时最多只有一个线程在accept()，限制了吞吐率。举个例子：
> 1. **kernel**: 收到2个连接请求。线程A和B同时等待，因为edge-triggered模式只会触发一次事件，假设线程A被唤起。
> 2. **thread A**: epoll_wait()结束，accept()成功。
> 3. **thread A**: epoll_ctl(EPOLL_CTL_MOD, EPOLLONESHOT)重新设置socket。
3. 套接字设置SO_REUSEPORT选项，各线程创建自己的listen socket监听同一个套接字端口，由内核处理连接请求到listen sockets的分发。但是这个方法也有个问题，当任意一个线程关闭listen socket，在accept queue里面等待的连接会被丢掉。（为什么会需要关闭listen socket呢？服务端reload或者负载不均衡reload时，线程退出关闭listen socket?）
内核4.4增加了SO_INCOMING_CPU选项，可以提升SO_REUSEPORT sockets的局部性，来自同一IP的连接请求转发给同一个listen socket。
内核4.5增加了SO_ATTACH_REUSEPORT_CBPF和SO_ATTACH_REUSEPORT_EBPF[套接字选项](http://man7.org/linux/man-pages/man7/socket.7.html)，可以替换掉SO_INCOMING_CPU，解决重新均衡时连接丢失的问题。设置这2个选项后，udp bind()/tcp listen()时，listen socket加入到这个监听端口的reuseport group，close()非group里面最后一个socket不会丢失等待accept的连接。

### 扩展read()
epoll除了用来扩展accept()，还可以用来扩展read()。想象这样的场景，大量的http客户请求，希望能尽快得到处理。每个请求处理时间不可预期，理想情况是把请求分成负载相同的块，每个线程处理一个块。更好是有一个"combined"队列，线程从这个队列里面取出请求处理。
我们用一个线程共享epoll fd，它监听所有的sockets请求，
#### epoll level triggered
level triggered模式有惊群效应，而且EPOLLEXCLUSIVE也解决不了多个线程对请求数据读取冲突。比如这种情况：
- **kernel**: 收到2047字节数据。
- **kernel**: 2个线程阻塞在epoll_wait()，假设内核唤醒线程A。
- **thread A**: epoll_wait()返回。
- **kernel**: 收到2个字节，内核唤起线程B。
- **thread A**: read(2048)读取2048字节数据。
- **thread B**: read(2048)读取1字节数据。

请求数据被2个线程读取，而且顺序信息也丢了。
#### epoll edge triggered
再看看边缘触发能不能解决？答案是不能。比如下面这种冲突场景：
- **kernel**: 收到2048字节数据。
- **kernel**: 2个线程阻塞在epoll_wait()，假设内核唤醒线程A。 
- **thread a**: epoll_wait()结束，read(2048)成功。
- **kernel**: 收到1个字节，唤起线程B。
- **thread b**: epoll_wait()结束，read(2048)读取1个字节。
- **thread a**: read(2048)返回EAGAIN。

#### 解决方案
使用edge-triggered模式，线程read()完整个请求之后设置EPOLLONESHOT。只能通过这种方式解决数据读取冲突。但是这个数据如果一直没读完，会阻塞其它socket事件的触发。
正确使用epoll很难。需要理解EPOLLONESHOT和EPOLLEXCLUSIVE，才能实现没有竞争的负载均衡。
考虑到EPOLLEXCLUSIVE是在epoll之后出现的，所以epoll最初设计并不是用来作多线程之间负载均衡的。

## 文件描述符vs文件描述
这会有多大事呢？且看。epoll事件是。
# kqueue
# windows IOCP
# 参考文献
- [select-is-fundamentally-broken](https://idea.popcount.org/2017-01-06-select-is-fundamentally-broken/)
- [epoll-is-fundamentally-broken-1/2](https://idea.popcount.org/2017-02-20-epoll-is-fundamentally-bproken-12/)
- [epoll-is-fundamentally-broken-2/2](https://idea.popcount.org/2017-03-20-epoll-is-fundamentally-broken-22/)
- [内核补丁增加EPOLLEXCLUSIVE标识](https://lwn.net/Articles/667087/)